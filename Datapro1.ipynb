{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class NormalAttention(nn.Module):\n",
    "    def __init__(self, d_input, d_target, d_hidden, dropout=0.1):\n",
    "        super(NormalAttention, self).__init__()\n",
    "        self.d_input = d_input\n",
    "        self.d_target = d_target\n",
    "        self.d_hid = d_hidden\n",
    "        self.attn = nn.Linear(d_input, d_hidden)\n",
    "        self.attn_target = nn.Linear(d_target, d_hidden)\n",
    "        # self.combine = nn.Linear(d_input + d_target, 1)\n",
    "        self.attn_target_1 = nn.Linear(d_hidden + d_hidden, d_hidden)\n",
    "        self.combine = nn.Linear(d_hidden, 1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        combine_input = self.attn(input_seq)\n",
    "        tar = self.attn_target(target_seq)\n",
    "        tar = tar.unsqueeze(1)\n",
    "        combine_tar = tar.view(len(input_seq), 1, -1)\n",
    "        _combine_input = torch.unsqueeze(combine_input, dim=1).expand(-1, 1, -1, -1)\n",
    "        _combine_tar = torch.unsqueeze(combine_tar, dim=2).expand(-1, -1, len(input_seq[0]), -1)\n",
    "\n",
    "        # _combine_input = torch.unsqueeze(input_seq, dim=1).expand(-1, 1, -1, -1)\n",
    "        # _combine_tar = torch.unsqueeze(tar, dim=2).expand(-1, -1, len(input_seq[0]), -1)\n",
    "\n",
    "        # _combine_tar = combine_tar.view(1, 1, 1, 50).expand(-1, -1, len(input_seq[1]), -1)\n",
    "\n",
    "        # attn_out = nn.Tanh(_combine_tar + _combine_input)\n",
    "        attn_out = nn.Tanh(self.attn_target_1(torch.cat((_combine_input, _combine_tar), dim=-1)))\n",
    "        attn_out = self.dropout(self.combine(attn_out))\n",
    "        attn_score = self.softmax(attn_out.squeeze(3))\n",
    "        # attn_out = input_seq * attn\n",
    "        # attn_out = attn_out.sum(dim=1)\n",
    "        out = torch.bmm(attn_score, input_seq)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, d_part1, d_part2, d_target, d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_part1 = d_part1\n",
    "        self.d_part2 = d_part2\n",
    "        self.d_hid = d_target\n",
    "        self.p1_tar_w = nn.Linear(d_part1, d_hidden)\n",
    "        self.p1_tar_u = nn.Linear(d_target, d_hidden)\n",
    "        self.p2_tar_w = nn.Linear(d_part2, d_hidden)\n",
    "        self.p2_tar_u = nn.Linear(d_target, d_hidden)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input1_seq, input2_seq, target):\n",
    "        p1_1 = self.p1_tar_w(input1_seq)\n",
    "        p1_2 = self.p1_tar_u(target)\n",
    "        p2_1 = self.p2_tar_w(input2_seq)\n",
    "        p2_2 = self.p2_tar_u(target)\n",
    "\n",
    "        z_l = nn.Tanh(p1_1 + p1_2)\n",
    "        z_r = nn.Tanh(p2_1 + p2_2)\n",
    "\n",
    "        z_w = torch.cat([z_l, z_r], dim=1)\n",
    "        z_w = self.softmax(z_w)\n",
    "\n",
    "        z_l_w = z_w[:, 0, :].unsqueeze(1)\n",
    "        z_r_w = z_w[:, 1, :].unsqueeze(1)\n",
    "\n",
    "        out = z_l_w * input1_seq + z_r_w * p2_1\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "\n",
    "    # Scaled-dot-product Attention layer\n",
    "\n",
    "    def __init__(self, d_query, d_key, d_value, mapping_on=\"query\"):\n",
    "\n",
    "        # mapping_on: whether linear transformation is required, mapping query or key into a new space\n",
    "        # mapping_on: \"query\" || \"key\" || \"both\" || \"none\"\n",
    "\n",
    "        super(DotProductAttention, self).__init__()\n",
    "\n",
    "        self.d_query = d_query\n",
    "        self.d_key = d_key\n",
    "        self.d_value = d_value\n",
    "        self.mapping_on = mapping_on\n",
    "\n",
    "        if mapping_on == \"query\":\n",
    "            # mapping query to key's space\n",
    "            self.q_h = nn.Linear(d_query, d_key)\n",
    "        elif mapping_on == \"key\":\n",
    "            # mapping key to query's space\n",
    "            self.k_h = nn.Linear(d_key, d_query)\n",
    "        elif mapping_on == \"both\":\n",
    "            # mapping query and key into the same space\n",
    "            self.q_h = nn.Linear(d_query, d_value)\n",
    "            self.k_h = nn.Linear(d_key, d_value)\n",
    "\n",
    "        self.temper = np.power(d_value, 0.5)\n",
    "        # self.weight = nn.Parameter(torch.Tensor(d_query, d_query))\n",
    "        # uniform = 1. / math.sqrt(self.d_query)\n",
    "        # self.weight.data.uniform_(-uniform, uniform)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "\n",
    "        # query: [s_batch, 1, d_query]\n",
    "        # key: [*, l_key, d_key] # usually d_key = d_query\n",
    "        # value: [*, l_value, d_value] # usually l_value = l_key\n",
    "        # if len(key.shape) == 3, then \"*\" must equal to s_batch\n",
    "\n",
    "        if self.mapping_on == \"query\":\n",
    "            q = self.q_h(q)\n",
    "        elif self.mapping_on == \"key\":\n",
    "            k = self.k_h(k)\n",
    "        elif self.mapping_on == \"both\":\n",
    "            q = self.q_h(q)\n",
    "            k = self.k_h(k)\n",
    "        # print(\"11\", k[0])\n",
    "        # [s_b, 1, d_q] * [*, d_k, l_k] = [s_b, 1, l_k]\n",
    "        if len(k.shape) == 3:\n",
    "            # similarity = torch.matmul(q, k.permute(0, 2, 1)) / self.temper\n",
    "            # similarity = torch.matmul(q, k.permute(0, 2, 1))\n",
    "            similarity = torch.matmul(q, k.permute(0, 2, 1))\n",
    "        else:\n",
    "            # len(k.shape) == 2\n",
    "            similarity = torch.matmul(q, k.transpose(0, 1)) / self.temper\n",
    "\n",
    "        # print(\"22\", similarity[0])\n",
    "        attn = f.softmax(similarity, dim=-1)\n",
    "        # print(\"attn : \", attn[1])\n",
    "        # [s_b, 1, l_k] * [*, l_v, d_v] = [s_b, 1, d_v]\n",
    "        output = torch.matmul(attn, v)\n",
    "        # print(\"44\", output[0])\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# import Datapro\n",
    "MAX_LENGTH = 300\n",
    "# BATCH_SIZE = 20\n",
    "# instructor = Datapro.Instructor()\n",
    "# embed = instructor.embedding\n",
    "device = torch.device(\"cuda\")\n",
    "final_embedding = np.array(np.load(\"/home/sysu502/Public/duxin/weakly/embedding/Vector_word_embedding_all.npy\"))\n",
    "# add = -1 + 2*np.random.random(300)\n",
    "add = np.zeros(300)\n",
    "final_embedding = np.row_stack((final_embedding, add))\n",
    "embed = torch.from_numpy(final_embedding)\n",
    "\n",
    "\n",
    "class WdeCnn(nn.Module):\n",
    "    def __init__(self, vector_size, hidden_dim, context_dim, dropout_p=0.1):\n",
    "        super(WdeCnn, self).__init__()\n",
    "        self.conv2d_h1 = nn.Conv2d(1, 200, (1, vector_size))\n",
    "        self.conv2d_h2 = nn.Conv2d(1, 200, (2, vector_size))\n",
    "        self.conv2d_h3 = nn.Conv2d(1, 200, (3, vector_size))\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.context_input = nn.Linear(context_dim, 100)\n",
    "        self.embedding_layer = nn.Linear(100 + hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.out = nn.Linear(hidden_dim + 100, hidden_dim)\n",
    "\n",
    "    def forward(self, input, context_input):\n",
    "        batch_len = input[:, 0]\n",
    "        batch_context = input[:, 1]\n",
    "        input_index = input[:, 2:]\n",
    "        input_index = input_index.long()\n",
    "        # seq_len = batch_len.item()\n",
    "        # input_index = input_index[0][0:seq_len]\n",
    "        # print('input_index',input_index)\n",
    "        input_value = self.embedding(input_index).view(BATCH_SIZE, 1, MAX_LENGTH, 300). float()\n",
    "        # print(input_value.size())\n",
    "\n",
    "        input_h1 = self.tanh(self.conv2d_h1(input_value))\n",
    "        input_h2 = self.tanh(self.conv2d_h2(input_value))\n",
    "        input_h3 = self.tanh(self.conv2d_h3(input_value))\n",
    "\n",
    "        input_h1 = F.max_pool2d(input_h1, (MAX_LENGTH, 1))\n",
    "        input_h2 = F.max_pool2d(input_h2, (MAX_LENGTH - 2 + 1, 1))\n",
    "        input_h3 = F.max_pool2d(input_h3, (MAX_LENGTH - 3 + 1, 1))\n",
    "\n",
    "        output_h1 = input_h1.view(BATCH_SIZE, 1, -1)\n",
    "        output_h2 = input_h2.view(BATCH_SIZE, 1, -1)\n",
    "        output_h3 = input_h3.view(BATCH_SIZE, 1, -1)\n",
    "\n",
    "        output = torch.cat((output_h1, output_h2), dim=2)\n",
    "        output = torch.cat((output, output_h3), dim=2)\n",
    "        output = self.tanh(self.hidden_layer(output))\n",
    "        context_input = context_input.view(BATCH_SIZE, 1, 50)\n",
    "        context_input = self.tanh(self.context_input(context_input))\n",
    "        embedding_input = torch.cat((output, context_input), dim=2)\n",
    "        output = self.out(embedding_input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class WdeRnnEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, context_dim):\n",
    "        super(WdeRnnEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.blstm = nn.LSTM(hidden_size, 300, bidirectional=True, batch_first=True)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.hidden_layer = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.context_input = nn.Linear(context_dim, 50)\n",
    "        self.embedding_layer = nn.Linear(50 + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, context_input):\n",
    "        BATCH_SIZE = len(input)\n",
    "        batch_len = input[:, 0]\n",
    "        batch_context = input[:, 1]\n",
    "        input_index = input[:, 2:]\n",
    "        input_index = input_index.long()\n",
    "        # seq_len = batch_len.item()\n",
    "        # input_index = input_index[0][0:seq_len]\n",
    "        # print('input_index',input_index)\n",
    "        # print(hidden.size())\n",
    "        sorted_seq_lengths, indices = torch.sort(batch_len, descending=True)\n",
    "        _, desorted_indices = torch.sort(indices, descending=False)\n",
    "        input_index = input_index[indices]\n",
    "        # input_value = self.embedding(input_index).view(BATCH_SIZE, MAX_LENGTH, 300).float()\n",
    "        input_value = self.embedding(input_index).view(BATCH_SIZE, MAX_LENGTH, 300).float()\n",
    "        packed_inputs = nn.utils.rnn.pack_padded_sequence(input_value, sorted_seq_lengths.cpu().data.numpy(), batch_first=True)\n",
    "\n",
    "        # print(sorted_seq_lengths, indices)\n",
    "        output, hidden = self.blstm(packed_inputs, hidden)\n",
    "        padded_res, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        desorted_output = padded_res[desorted_indices]\n",
    "        desorted_output = F.max_pool2d(desorted_output, (desorted_output.size(1), 1))\n",
    "\n",
    "        # output.view(self.hidden_size * 2, -1)\n",
    "        # output = torch.max(output)\n",
    "        desorted_output = self.tanh(self.hidden_layer(desorted_output))\n",
    "\n",
    "        context_input = context_input.view(BATCH_SIZE, 1, 50)\n",
    "        context_input = self.tanh(self.context_input(context_input))\n",
    "\n",
    "        embedding_input = torch.cat((desorted_output, context_input), dim=2)\n",
    "        desorted_output = self.tanh(self.embedding_layer(embedding_input))\n",
    "        return desorted_output\n",
    "\n",
    "    def initHidden(self, BATCH_SIZE):\n",
    "        return (torch.zeros(2, BATCH_SIZE, self.hidden_size, device=device),\n",
    "                torch.zeros(2, BATCH_SIZE, self.hidden_size, device=device))\n",
    "\n",
    "\n",
    "class WdeRnnDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_rate, context_dim):\n",
    "        super(WdeRnnDecoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.hidden_layer = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.context_input = nn.Linear(context_dim, 100)\n",
    "        self.embedding_layer = nn.Linear(100 + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, context):\n",
    "        input.view(self.hidden_size * 2, -1)\n",
    "        output = torch.max(input)\n",
    "        output.view(1, -1)\n",
    "        output = self.tanh(self.hidden_layer(input))\n",
    "        output = self.tanh(self.context_input(output))\n",
    "        output = self.tanh(self.embedding_layer(output))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(2, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class SoftMaxOutput(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SoftMaxOutput, self).__init__()\n",
    "\n",
    "        self.embedding_layer = nn.Linear(hidden_size, 150)\n",
    "        self.Classification_layer = nn.Linear(150, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        output = model(input)\n",
    "        output = self.embedding_layer(output)\n",
    "        output = self.Classification_layer(output)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class WdeRnnEncoderFix(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, context_dim, trained_aspect, dropout=0.1):\n",
    "        super(WdeRnnEncoderFix, self).__init__()\n",
    "#         self.opt = opt\n",
    "        self.hidden_size = hidden_size\n",
    "        self.blstm = nn.LSTM(hidden_size, 300, bidirectional=True, batch_first=True)\n",
    "        self.embedded = nn.Embedding.from_pretrained(embed)\n",
    "        self.aspect_embed = nn.Embedding.from_pretrained(trained_aspect)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.hidden_layer = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.context_input_ = nn.Linear(600, 50)\n",
    "        self.embedding_layers = nn.Linear(0 + hidden_size, output_size)\n",
    "        # self.slf_attention = attention.MultiHeadAttention(600, 3)\n",
    "        # self.slf_attention = attention.MultiHeadAttentionDotProduct(3, 600, 300, 300, 0.01)\n",
    "        # self.Position_wise = attention.PositionwiseFeedForward(600, 600, 0.01)\n",
    "        self.attention = NormalAttention(600, 50, 50)\n",
    "        self.gate = Gate(300, 50, 50, 300)\n",
    "        \n",
    "        self.min_context = nn.Linear(300, 50)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        BATCH_SIZE = len(input)\n",
    "        batch_len = input[:, 0]\n",
    "        batch_context = input[:, 1]\n",
    "        input_index = input[:, 2:]\n",
    "        input_index = input_index.long()\n",
    "        # seq_len = batch_len.item()\n",
    "        # input_index = input_index[0][0:seq_len]\n",
    "        # print('input_index',input_index)\n",
    "        # print(hidden.size())\n",
    "        sorted_seq_lengths, indices = torch.sort(batch_len, descending=True)\n",
    "        input_index = input_index[:, 0: sorted_seq_lengths[0]]\n",
    "        _, desorted_indices = torch.sort(indices, descending=False)\n",
    "        input_index = input_index[indices]\n",
    "        input_value = self.embedded(input_index)\n",
    "        input_value = input_value.float()\n",
    "        packed_inputs = nn.utils.rnn.pack_padded_sequence(input_value, sorted_seq_lengths.cpu().data.numpy()\n",
    "                                                          , batch_first=True)\n",
    "\n",
    "        # print(sorted_seq_lengths, indices)\n",
    "        output, hidden = self.blstm(packed_inputs, hidden)\n",
    "        padded_res, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        desorted_output = padded_res[desorted_indices]\n",
    "\n",
    "        '''\n",
    "        self attention module add or not?\n",
    "        point wise product add or not?\n",
    "        '''\n",
    "        # desorted_output = self.slf_attention(desorted_output, context_input)\n",
    "        # desorted_output, _ = self.slf_attention(desorted_output, desorted_output, desorted_output)\n",
    "        # desorted_output = self.Position_wise(desorted_output)\n",
    "\n",
    "        '''\n",
    "        Normal attention module add or not?\n",
    "        '''\n",
    "        \n",
    "        context_input = self.aspect_embed(batch_context).float()\n",
    "        context_input = self.min_context(context_input)\n",
    "\n",
    "        attn_target = self.attention(desorted_output, context_input)\n",
    "\n",
    "        desorted_output = F.max_pool2d(desorted_output, (desorted_output.size(1), 1))\n",
    "\n",
    "        # output.view(self.hidden_size * 2, -1)\n",
    "        # output = torch.max(output)\n",
    "        desorted_output = self.tanh(self.hidden_layer(desorted_output))\n",
    "\n",
    "        context_input = context_input.view(BATCH_SIZE, 1, 50)\n",
    "        _context_input = self.tanh(self.context_input_(attn_target))\n",
    "\n",
    "        gate_out = self.gate(desorted_output, _context_input, context_input)\n",
    "\n",
    "        embedding_input = torch.cat((desorted_output, _context_input), dim=2)\n",
    "        desorted_output = self.tanh(self.embedding_layers(gate_out))\n",
    "        return desorted_output\n",
    "\n",
    "    def initHidden(self, BATCH_SIZE):\n",
    "        return (torch.zeros(2, BATCH_SIZE, self.hidden_size, device=device),\n",
    "                torch.zeros(2, BATCH_SIZE, self.hidden_size, device=device))\n",
    "    \n",
    "\n",
    "class PreTrainABAE_fix(nn.Module):\n",
    "    def __init__(self, embed_dim, n_aspect, aspect_embedding):\n",
    "\n",
    "        super(PreTrainABAE_fix, self).__init__()\n",
    "        # self.opt = opt\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_aspect = n_aspect\n",
    "        self.embedded = nn.Embedding.from_pretrained(embed)\n",
    "\n",
    "        # query: global_content_embeding: [batch_size, embed_dim]\n",
    "    # key: inputs: [batch_size, doc_size, embed_dim]\n",
    "        # value: inputs\n",
    "        # mapping the input word embedding to global_content_embedding space\n",
    "        self.sentence_embedding_attn = DotProductAttention(\n",
    "            d_query=embed_dim,\n",
    "            d_key=embed_dim,\n",
    "            d_value=embed_dim,\n",
    "            mapping_on=\"key\"\n",
    "        )\n",
    "\n",
    "        # embed_dim => n_aspect\n",
    "        self.aspect_linear = nn.Linear(embed_dim, n_aspect)\n",
    "\n",
    "        # initialized with the centroids of clusters resulting from running k-means on word embeddings in corpus\n",
    "        self.aspect_lookup_mat = nn.Parameter(data=aspect_embedding, requires_grad=True)\n",
    "        # self.aspect_lookup_mat = nn.Parameter(torch.Tensor(n_aspect, embed_dim).double())\n",
    "        # self.aspect_lookup_mat.data.uniform_(-1, 1)\n",
    "\n",
    "    def forward(self, inputs, eps=1e-06):\n",
    "        input_lengths = inputs[:, 0]\n",
    "        inputs = inputs[:, 2:]\n",
    "        input_index = inputs.long()\n",
    "        sorted_seq_lengths, indices = torch.sort(input_lengths, descending=True)\n",
    "        _, desorted_indices = torch.sort(indices, descending=False)\n",
    "        input_index = input_index[:, 0: sorted_seq_lengths[0]]\n",
    "        # input_index = input_index[indices]\n",
    "        inputs = self.embedded(input_index).double()\n",
    "\n",
    "        # inputs: [batch_size, doc_size, embed_dim]\n",
    "        # input_lengths: [batch_size]\n",
    "        # averaging embeddings in a document: [batch_size, 1, embed_dim]\n",
    "        avg_denominator = input_lengths.repeat(self.embed_dim).view(self.embed_dim, -1).transpose(0, 1).float()\n",
    "        global_content_embed = torch.sum(inputs.double(), dim=1).div(avg_denominator.double())\n",
    "        global_content_embed = global_content_embed.unsqueeze(dim=1)\n",
    "\n",
    "        # construct sentence embedding, with attention(query: global_content_embed, keys: inputs, value: inputs)\n",
    "        # [batch_size, embed_dim]\n",
    "        sentence_embedding, _ = self.sentence_embedding_attn(\n",
    "            global_content_embed.float(), inputs.float(), inputs.float()\n",
    "        )\n",
    "        # print(\"attn : \", sentence_embedding)\n",
    "        sentence_embedding = sentence_embedding.squeeze(dim=1)\n",
    "\n",
    "        # [batch_size, n_aspect]\n",
    "        aspect_weight = F.softmax(self.aspect_linear(sentence_embedding), dim=1)\n",
    "\n",
    "        _, predicted = torch.max(aspect_weight.data, 1)\n",
    "\n",
    "        return predicted\n",
    "\n",
    "    def regular(self, eps=1e-06):\n",
    "        div = eps + torch.norm(self.aspect_lookup_mat, 2, -1)\n",
    "        div = div.view(-1, 1)\n",
    "        self.aspect_lookup_mat.data = self.aspect_lookup_mat / div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Take Word To Vec\n",
      "epoch 0 of 200: TEST : 1.0072120765965817\n",
      "epoch 0 of 200: loss : 3.9660377502441406\n",
      "epoch 1 of 200: loss : 4.02294921875\n",
      "epoch 2 of 200: TEST : 1.0925715233800144\n",
      "epoch 2 of 200: loss : 3.3201067447662354\n",
      "epoch 3 of 200: loss : 3.181401491165161\n",
      "epoch 4 of 200: TEST : 1.1620985738252738\n",
      "epoch 4 of 200: loss : 3.26112961769104\n",
      "epoch 5 of 200: loss : 2.9958384037017822\n",
      "epoch 6 of 200: TEST : 1.2202129753386586\n",
      "epoch 6 of 200: loss : 2.752567768096924\n",
      "epoch 7 of 200: loss : 2.530390977859497\n",
      "epoch 8 of 200: TEST : 1.2066817858838796\n",
      "epoch 8 of 200: loss : 2.8767051696777344\n",
      "epoch 9 of 200: loss : 2.1338565349578857\n",
      "epoch 10 of 200: TEST : 1.2422554589488093\n",
      "epoch 10 of 200: loss : 2.5703446865081787\n",
      "epoch 11 of 200: loss : 2.1935315132141113\n",
      "epoch 12 of 200: TEST : 1.2207305760029192\n",
      "epoch 12 of 200: loss : 2.5851030349731445\n",
      "epoch 13 of 200: loss : 2.4825611114501953\n",
      "epoch 14 of 200: TEST : 1.2198385477826583\n",
      "epoch 14 of 200: loss : 2.2654356956481934\n",
      "epoch 15 of 200: loss : 2.6369707584381104\n",
      "epoch 16 of 200: TEST : 1.1982993481116946\n",
      "epoch 16 of 200: loss : 2.896411657333374\n",
      "epoch 17 of 200: loss : 2.2096447944641113\n",
      "epoch 18 of 200: TEST : 1.2079542083251997\n",
      "epoch 18 of 200: loss : 1.658728837966919\n",
      "epoch 19 of 200: loss : 1.953510046005249\n",
      "epoch 20 of 200: TEST : 1.2101807117826617\n",
      "epoch 20 of 200: loss : 1.9262652397155762\n",
      "epoch 21 of 200: loss : 2.0346853733062744\n",
      "epoch 22 of 200: TEST : 1.1923786357678303\n",
      "epoch 22 of 200: loss : 1.7746690511703491\n",
      "epoch 23 of 200: loss : 1.6546685695648193\n",
      "epoch 24 of 200: TEST : 1.197938972611749\n",
      "epoch 24 of 200: loss : 2.1142196655273438\n",
      "epoch 25 of 200: loss : 1.6743041276931763\n",
      "epoch 26 of 200: TEST : 1.1936636205833147\n",
      "epoch 26 of 200: loss : 1.331350564956665\n",
      "epoch 27 of 200: loss : 1.855718731880188\n",
      "epoch 28 of 200: TEST : 1.1848733198717558\n",
      "epoch 28 of 200: loss : 1.378355860710144\n",
      "epoch 29 of 200: loss : 1.531569242477417\n",
      "epoch 30 of 200: TEST : 1.1840446188806995\n",
      "epoch 30 of 200: loss : 1.9747142791748047\n",
      "epoch 31 of 200: loss : 1.0759104490280151\n",
      "epoch 32 of 200: TEST : 1.1981367647666596\n",
      "epoch 32 of 200: loss : 1.4666272401809692\n",
      "epoch 33 of 200: loss : 1.1992868185043335\n",
      "epoch 34 of 200: TEST : 1.1687886836370074\n",
      "epoch 34 of 200: loss : 1.3639715909957886\n",
      "epoch 35 of 200: loss : 1.0771584510803223\n",
      "epoch 36 of 200: TEST : 1.1823701046732282\n",
      "epoch 36 of 200: loss : 1.1999937295913696\n",
      "epoch 37 of 200: loss : 1.2343131303787231\n",
      "epoch 38 of 200: TEST : 1.185780228453214\n",
      "epoch 38 of 200: loss : 1.24428391456604\n",
      "epoch 39 of 200: loss : 0.8285762667655945\n",
      "epoch 40 of 200: TEST : 1.1832444618974696\n",
      "epoch 40 of 200: loss : 1.1129084825515747\n",
      "epoch 41 of 200: loss : 0.8208948373794556\n",
      "epoch 42 of 200: TEST : 1.189688399826087\n",
      "epoch 42 of 200: loss : 0.9934582710266113\n",
      "epoch 43 of 200: loss : 0.5998724102973938\n",
      "epoch 44 of 200: TEST : 1.1815478497666205\n",
      "epoch 44 of 200: loss : 1.2893165349960327\n",
      "epoch 45 of 200: loss : 0.7078795433044434\n",
      "epoch 46 of 200: TEST : 1.1816700793735095\n",
      "epoch 46 of 200: loss : 1.2300387620925903\n",
      "epoch 47 of 200: loss : 0.5717966556549072\n",
      "epoch 48 of 200: TEST : 1.1872226230269614\n",
      "epoch 48 of 200: loss : 0.9329690337181091\n",
      "epoch 49 of 200: loss : 0.5392729640007019\n",
      "epoch 50 of 200: TEST : 1.190286755113569\n",
      "epoch 50 of 200: loss : 0.3883955478668213\n",
      "epoch 51 of 200: loss : 0.5345791578292847\n",
      "epoch 52 of 200: TEST : 1.1981155926362348\n",
      "epoch 52 of 200: loss : 0.9220829606056213\n",
      "epoch 53 of 200: loss : 0.3711780905723572\n",
      "epoch 54 of 200: TEST : 1.1886517047469378\n",
      "epoch 54 of 200: loss : 0.8344323039054871\n",
      "epoch 55 of 200: loss : 0.36516863107681274\n",
      "epoch 56 of 200: TEST : 1.1895174676534284\n",
      "epoch 56 of 200: loss : 0.7540813088417053\n",
      "epoch 57 of 200: loss : 0.3257352411746979\n",
      "epoch 58 of 200: TEST : 1.1930612771390054\n",
      "epoch 58 of 200: loss : 0.5839092135429382\n",
      "epoch 59 of 200: loss : 0.39057958126068115\n",
      "epoch 60 of 200: TEST : 1.1905357474403058\n",
      "epoch 60 of 200: loss : 0.3686142563819885\n",
      "epoch 61 of 200: loss : 0.6345921158790588\n",
      "epoch 62 of 200: TEST : 1.1854519175997498\n",
      "epoch 62 of 200: loss : 0.49288105964660645\n",
      "epoch 63 of 200: loss : 0.28526854515075684\n",
      "epoch 64 of 200: TEST : 1.190490380626337\n",
      "epoch 64 of 200: loss : 0.36792832612991333\n",
      "epoch 65 of 200: loss : 0.18848535418510437\n",
      "epoch 66 of 200: TEST : 1.187762848059771\n",
      "epoch 66 of 200: loss : 0.42886343598365784\n",
      "epoch 67 of 200: loss : 0.48745161294937134\n",
      "epoch 68 of 200: TEST : 1.189890129243626\n",
      "epoch 68 of 200: loss : 0.4364408850669861\n",
      "epoch 69 of 200: loss : 0.4380149245262146\n",
      "epoch 70 of 200: TEST : 1.18368375816902\n",
      "epoch 70 of 200: loss : 0.39194899797439575\n",
      "epoch 71 of 200: loss : 0.38961881399154663\n",
      "epoch 72 of 200: TEST : 1.1860619471249785\n",
      "epoch 72 of 200: loss : 0.4737577736377716\n",
      "epoch 73 of 200: loss : 0.263942152261734\n",
      "epoch 74 of 200: TEST : 1.1786591357493192\n",
      "epoch 74 of 200: loss : 0.3181774914264679\n",
      "epoch 75 of 200: loss : 0.532684862613678\n",
      "epoch 76 of 200: TEST : 1.1908859601941673\n",
      "epoch 76 of 200: loss : 0.3862733542919159\n",
      "epoch 77 of 200: loss : 0.2947344183921814\n",
      "epoch 78 of 200: TEST : 1.1885260258297525\n",
      "epoch 78 of 200: loss : 0.5882554650306702\n",
      "epoch 79 of 200: loss : 0.32923755049705505\n",
      "epoch 80 of 200: TEST : 1.178454975638174\n",
      "epoch 80 of 200: loss : 0.403866708278656\n",
      "epoch 81 of 200: loss : 0.19994176924228668\n",
      "epoch 82 of 200: TEST : 1.185165279538458\n",
      "epoch 82 of 200: loss : 0.16417944431304932\n",
      "epoch 83 of 200: loss : 0.17893047630786896\n",
      "epoch 84 of 200: TEST : 1.1785093322763673\n",
      "epoch 84 of 200: loss : 0.2056477665901184\n",
      "epoch 85 of 200: loss : 0.07844783365726471\n",
      "epoch 86 of 200: TEST : 1.1776171009547634\n",
      "epoch 86 of 200: loss : 0.23125645518302917\n",
      "epoch 87 of 200: loss : 0.2845531105995178\n",
      "epoch 88 of 200: TEST : 1.1733841166058179\n",
      "epoch 88 of 200: loss : 0.0943206176161766\n",
      "epoch 89 of 200: loss : 0.18773147463798523\n",
      "epoch 90 of 200: TEST : 1.1773832459187783\n",
      "epoch 90 of 200: loss : 0.2918405830860138\n",
      "epoch 91 of 200: loss : 0.1215413510799408\n",
      "epoch 92 of 200: TEST : 1.186693864259099\n",
      "epoch 92 of 200: loss : 0.2341625988483429\n",
      "epoch 93 of 200: loss : 0.06419915705919266\n",
      "epoch 94 of 200: TEST : 1.1760065136317586\n",
      "epoch 94 of 200: loss : 0.14053773880004883\n",
      "epoch 95 of 200: loss : 0.07984385639429092\n",
      "epoch 96 of 200: TEST : 1.181302486871124\n",
      "epoch 96 of 200: loss : 0.1429913341999054\n",
      "epoch 97 of 200: loss : 0.03418546915054321\n",
      "epoch 98 of 200: TEST : 1.1795019779277536\n",
      "epoch 98 of 200: loss : 0.116994708776474\n",
      "epoch 99 of 200: loss : 0.15304134786128998\n",
      "epoch 100 of 200: TEST : 1.1833283313691776\n",
      "epoch 100 of 200: loss : 0.2697264850139618\n",
      "epoch 101 of 200: loss : 0.07045134156942368\n",
      "epoch 102 of 200: TEST : 1.1854089251243976\n",
      "epoch 102 of 200: loss : 0.055405035614967346\n",
      "epoch 103 of 200: loss : 0.05996542423963547\n",
      "epoch 104 of 200: TEST : 1.1821521057210678\n",
      "epoch 104 of 200: loss : 0.06784062832593918\n",
      "epoch 105 of 200: loss : 0.04131901264190674\n",
      "epoch 106 of 200: TEST : 1.1811414110217993\n",
      "epoch 106 of 200: loss : 0.3066088557243347\n",
      "epoch 107 of 200: loss : 0.039464905858039856\n",
      "epoch 108 of 200: TEST : 1.1786594190896182\n",
      "epoch 108 of 200: loss : 0.020674556493759155\n",
      "epoch 109 of 200: loss : 0.08343228697776794\n",
      "epoch 110 of 200: TEST : 1.1719304505061305\n",
      "epoch 110 of 200: loss : 0.1693471074104309\n",
      "epoch 111 of 200: loss : 0.03698020428419113\n",
      "epoch 112 of 200: TEST : 1.178668794991602\n",
      "epoch 112 of 200: loss : 0.11235931515693665\n",
      "epoch 113 of 200: loss : 0.04518815129995346\n",
      "epoch 114 of 200: TEST : 1.1699691892189443\n",
      "epoch 114 of 200: loss : 0.05981314182281494\n",
      "epoch 115 of 200: loss : 0.004636324942111969\n",
      "epoch 116 of 200: TEST : 1.1723205998312234\n",
      "epoch 116 of 200: loss : 0.0670594722032547\n",
      "epoch 117 of 200: loss : 0.06998638063669205\n",
      "epoch 118 of 200: TEST : 1.1734098629099443\n",
      "epoch 118 of 200: loss : 0.13290098309516907\n",
      "epoch 119 of 200: loss : 0.06141055375337601\n",
      "epoch 120 of 200: TEST : 1.1747085609875694\n",
      "epoch 120 of 200: loss : 0.08681011945009232\n",
      "epoch 121 of 200: loss : 0.05299481004476547\n",
      "epoch 122 of 200: TEST : 1.1726941412783076\n",
      "epoch 122 of 200: loss : 0.02969656139612198\n",
      "epoch 123 of 200: loss : 0.020565181970596313\n",
      "epoch 124 of 200: TEST : 1.1753056402678193\n",
      "epoch 124 of 200: loss : 0.04597615450620651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 125 of 200: loss : 0.03890202194452286\n",
      "epoch 126 of 200: TEST : 1.1756061951732975\n",
      "epoch 126 of 200: loss : 0.002786703407764435\n",
      "epoch 127 of 200: loss : 0.015179276466369629\n",
      "epoch 128 of 200: TEST : 1.1725526405906692\n",
      "epoch 128 of 200: loss : 0.014219522476196289\n",
      "epoch 129 of 200: loss : 0.012026891112327576\n",
      "epoch 130 of 200: TEST : 1.1733603995394737\n",
      "epoch 130 of 200: loss : 0.025490883737802505\n",
      "epoch 131 of 200: loss : 0.021845534443855286\n",
      "epoch 132 of 200: TEST : 1.170932191422295\n",
      "epoch 132 of 200: loss : 0.025328144431114197\n",
      "epoch 133 of 200: loss : 0.006071664392948151\n",
      "epoch 134 of 200: TEST : 1.1692036072036174\n",
      "epoch 134 of 200: loss : 0.06074149161577225\n",
      "epoch 135 of 200: loss : 0.044429123401641846\n",
      "epoch 136 of 200: TEST : 1.1722897214257502\n",
      "epoch 136 of 200: loss : 0.011827819049358368\n",
      "epoch 137 of 200: loss : 0.009379923343658447\n",
      "epoch 138 of 200: TEST : 1.1756918777507135\n",
      "epoch 138 of 200: loss : 0.0\n",
      "epoch 139 of 200: loss : 0.013074927031993866\n",
      "epoch 140 of 200: TEST : 1.169042184052949\n",
      "epoch 140 of 200: loss : 0.10223761200904846\n",
      "epoch 141 of 200: loss : 0.0007354319095611572\n",
      "epoch 142 of 200: TEST : 1.1751148799636988\n",
      "epoch 142 of 200: loss : 0.0\n",
      "epoch 143 of 200: loss : 0.0\n",
      "epoch 144 of 200: TEST : 1.1696996618195596\n",
      "epoch 144 of 200: loss : 0.019627943634986877\n",
      "epoch 145 of 200: loss : 0.0018285363912582397\n",
      "epoch 146 of 200: TEST : 1.1736752913290411\n",
      "epoch 146 of 200: loss : 0.012857824563980103\n",
      "epoch 147 of 200: loss : 0.03274726867675781\n",
      "epoch 148 of 200: TEST : 1.1686027780931618\n",
      "epoch 148 of 200: loss : 0.003806978464126587\n",
      "epoch 149 of 200: loss : 0.011109329760074615\n",
      "epoch 150 of 200: TEST : 1.1706142651920177\n",
      "epoch 150 of 200: loss : 0.004557117819786072\n",
      "epoch 151 of 200: loss : 0.0010758638381958008\n",
      "epoch 152 of 200: TEST : 1.1694968502619987\n",
      "epoch 152 of 200: loss : 0.0026813745498657227\n",
      "epoch 153 of 200: loss : 0.0\n",
      "epoch 154 of 200: TEST : 1.1710723552298596\n",
      "epoch 154 of 200: loss : 0.0030209124088287354\n",
      "epoch 155 of 200: loss : 0.0\n",
      "epoch 156 of 200: TEST : 1.1689683746864283\n",
      "epoch 156 of 200: loss : 0.08093342185020447\n",
      "epoch 157 of 200: loss : 0.0\n",
      "epoch 158 of 200: TEST : 1.1718782351182717\n",
      "epoch 158 of 200: loss : 0.06582311540842056\n",
      "epoch 159 of 200: loss : 0.009540706872940063\n",
      "epoch 160 of 200: TEST : 1.1693265945616698\n",
      "epoch 160 of 200: loss : 0.02190488576889038\n",
      "epoch 161 of 200: loss : 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1df023415dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0minstructor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;31m# instructor.beginTrain()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m     \u001b[0minstructor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeginTrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1df023415dcf>\u001b[0m in \u001b[0;36mbeginTrain_lstm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mvalid_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreTrainABAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_now\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1df023415dcf>\u001b[0m in \u001b[0;36mvalid\u001b[0;34m(self, PreTrainABAE, model_trained)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                     \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0mpos_dis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdis\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m     return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 1882\u001b[0;31m                          out=out, **kwargs)\n\u001b[0m\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "batch_size = 128\n",
    "sample_size = 100000\n",
    "\n",
    "\n",
    "class DataProcess(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class Instructor:\n",
    "    def __init__(self):\n",
    "        all_data, embedding, self.pos_test, self.neg_test = push()\n",
    "        self.embedding = embedding\n",
    "        self.train_data_loader = DataLoader(\n",
    "            dataset=all_data,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=8\n",
    "        )\n",
    "\n",
    "    def valid(self, PreTrainABAE, model_trained):\n",
    "        PreTrainABAE = PreTrainABAE.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pos_len = len(self.pos_test)\n",
    "            neg_len = len(self.neg_test)\n",
    "            #  print(\"1\")\n",
    "            run_hidden = model_trained.initHidden(1)\n",
    "            for idx, sentence in enumerate(self.pos_test):\n",
    "                if idx == 0:\n",
    "                    sentence = torch.from_numpy(sentence).view(1, -1).cuda()\n",
    "                    sentence[:, 1] = PreTrainABAE(sentence)\n",
    "                    pos_embedding = model_trained(sentence, run_hidden).view(1, 300)\n",
    "                else:\n",
    "                    sentence = torch.from_numpy(sentence).view(1, -1).cuda()\n",
    "                    sentence[:, 1] = PreTrainABAE(sentence)\n",
    "                    pos_embedding = torch.cat((\n",
    "                        pos_embedding,\n",
    "                        model_trained(sentence, run_hidden).view(1, 300)\n",
    "                        ),\n",
    "                        dim=0\n",
    "                    )\n",
    "            for idx, sentence in enumerate(self.neg_test):\n",
    "                if idx == 0:\n",
    "                    sentence = torch.from_numpy(sentence).view(1, -1).cuda()\n",
    "                    sentence[:, 1] = PreTrainABAE(sentence)\n",
    "                    neg_embedding = model_trained(sentence, run_hidden).view(1, 300)\n",
    "                else:\n",
    "                    sentence = torch.from_numpy(sentence).view(1, -1).cuda()\n",
    "                    sentence[:, 1] = PreTrainABAE(sentence)\n",
    "                    neg_embedding = torch.cat((\n",
    "                        neg_embedding,\n",
    "                        model_trained(sentence, run_hidden).view(1, 300)\n",
    "                        ),\n",
    "                        dim=0\n",
    "                    )\n",
    "\n",
    "            pos_embedding = pos_embedding.cpu()\n",
    "            neg_embedding = neg_embedding.cpu()\n",
    "            pos_embedding = pos_embedding.detach().numpy()\n",
    "            neg_embedding = neg_embedding.detach().numpy()\n",
    "            \n",
    "            neg_dis = 0\n",
    "            pos_count = 0\n",
    "            neg_count = 0\n",
    "            for idx, sentence1 in enumerate(pos_embedding[:-1]):\n",
    "                for sentence2 in pos_embedding[idx + 1:]:\n",
    "                    dis = sentence1 - sentence2\n",
    "                    dis = dis * dis\n",
    "                    dis = np.sum(dis)\n",
    "                    dis = dis ** 0.5\n",
    "                    pos_dis += dis ** 0.5\n",
    "                    pos_count += 1\n",
    "    #         print(\"4\")\n",
    "            for idx, sentence1 in enumerate(neg_embedding[:-1]):\n",
    "                for sentence2 in neg_embedding[idx + 1:]:\n",
    "                    dis = sentence1 - sentence2\n",
    "                    dis = dis * dis\n",
    "                    dis = np.sum(dis)\n",
    "                    dis = dis ** 0.5\n",
    "                    neg_dis += dis ** 0.5\n",
    "                    neg_count += 1\n",
    "                    \n",
    "            valid_intra = (pos_dis + neg_dis) / (pos_count + neg_count)\n",
    "\n",
    "            inter_dis = 0\n",
    "            for idx, sentence1 in enumerate(pos_embedding):\n",
    "                for sentence2 in neg_embedding:\n",
    "                    dis = sentence1 - sentence2\n",
    "                    dis = dis * dis\n",
    "                    dis = np.sum(dis)\n",
    "                    dis = dis ** 0.5\n",
    "                    inter_dis += dis ** 0.5\n",
    "\n",
    "            valid_inter = inter_dis / (pos_len * neg_len)\n",
    "    #         print(\"6\")\n",
    "        return valid_inter / valid_intra\n",
    "\n",
    "    def cal_distence(self, sentence1, sentence2):\n",
    "        distance = torch.dist(sentence1, sentence2, 2)\n",
    "        return distance\n",
    "\n",
    "    def beginTrain_lstm(self):\n",
    "        \n",
    "        init_aspect = np.array(np.load(\"/home/sysu502/Public/duxin/weakly/initAspect.npy\"))\n",
    "        # init_aspect = init_aspect / np.linalg.norm(init_aspect, axis=-1, keepdims=True)\n",
    "        init_aspect = torch.from_numpy(init_aspect)\n",
    "        PreTrainABAE = PreTrainABAE_fix(300, 24, init_aspect).cuda()\n",
    "    \n",
    "        pre_trained_aspect = torch.load(\"/home/sysu502/Public/duxin/weakly/AspectExtract/Aspect_Model.pkl\")\n",
    "        aspect_dict = PreTrainABAE.state_dict()\n",
    "        pre_trained_dict = {k: v for k, v in pre_trained_aspect.items() if k in aspect_dict}\n",
    "        aspect_dict.update(pre_trained_dict)\n",
    "        PreTrainABAE.load_state_dict(aspect_dict)\n",
    "        PreTrainABAE = PreTrainABAE.eval()\n",
    "\n",
    "        trained_aspect = pre_trained_aspect[\"aspect_lookup_mat\"].data\n",
    "        run = WdeRnnEncoderFix(300, 300, 50, trained_aspect)\n",
    "        run = run.cuda()\n",
    "        params = []\n",
    "        for param in run.parameters():\n",
    "            if param.requires_grad:\n",
    "                params.append(param)\n",
    "        loss_func = nn.TripletMarginLoss(margin=4.0, p=2)\n",
    "        optimizer = optim.SGD(params, lr=0.0001)\n",
    "\n",
    "        for epoch in range(200):\n",
    "            run_hidden = run.initHidden(batch_size)\n",
    "            loss_last = torch.tensor([0], dtype=torch.float)\n",
    "            \n",
    "            for idx, sample_batch in enumerate(self.train_data_loader):\n",
    "                now = time.time()\n",
    "                run = run.train()\n",
    "                input1 = sample_batch['input1'].cuda()\n",
    "                input2 = sample_batch['input2'].cuda()\n",
    "                input3 = sample_batch['input3'].cuda()\n",
    "                # if input1[:,0].item() < 3 or input2[:,0].item() < 3 or input3[:,0].item() < 3:\n",
    "                #     continue\n",
    "                aspect_info = PreTrainABAE(input1)\n",
    "                input1[:, 1] = aspect_info\n",
    "                aspect_info = PreTrainABAE(input2)\n",
    "                input2[:, 1] = aspect_info\n",
    "                aspect_info = PreTrainABAE(input3)\n",
    "                input3[:, 1] = aspect_info\n",
    "                out1 = run(input1.cuda(), run_hidden).view(batch_size, 300)\n",
    "                out2 = run(input2.cuda(), run_hidden).view(batch_size, 300)\n",
    "                out3 = run(input3.cuda(), run_hidden).view(batch_size, 300)\n",
    "\n",
    "                loss_last = loss_func(out1, out2, out3)\n",
    "                loss_last.backward()\n",
    "                optimizer.step()\n",
    "            if epoch % 2 == 0:\n",
    "                run.zero_grad()\n",
    "                run = run.eval()\n",
    "                valid_now = self.valid(PreTrainABAE, run)\n",
    "                a = round((loss_last).item(), 5)\n",
    "                b = round(valid_now, 5)\n",
    "                if valid_now > 1.13:\n",
    "                    file_name = \"/media/sysu502/data/bcfox_model/\" +\"every2_loss_\" + str(a) + \"valid_\" + str(b) + \".pkl\"\n",
    "                    torch.save(run.state_dict(), file_name)\n",
    "                valid_compare = valid_now\n",
    "\n",
    "                print('epoch {} of {}: TEST : {}'.format(epoch, 200, valid_now))\n",
    "            print('epoch {} of {}: loss : {}'.format(epoch, 200, (loss_last).item()))\n",
    "\n",
    "\n",
    "def push():\n",
    "    def normalizeString(s):\n",
    "        s = unicodeToAscii(s.lower().strip())\n",
    "        s = re.sub(r\"([.!?\\(\\)\\\"])\", r\"\", s)\n",
    "        s = re.sub(r\"[^0-9a-zA-Z]+\", r\" \", s)\n",
    "        return s\n",
    "\n",
    "    def normalize(s):\n",
    "        # s = unicodeToAscii(s.strip())\n",
    "        s = re.sub(r\"([\\[\\]\\\"\\n])\", r\"\", s)\n",
    "        return s\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                       if unicodedata.category(c) != 'Mn'\n",
    "                       )\n",
    "\n",
    "    lines_pos1 = open('/home/sysu502/Public/duxin/weakly/Large_Scale_Sentiment_Classification_Data_v0.2/Weakly_labeled_data_1.1M/camera_positive.csv'\n",
    "                     , encoding='utf-8').read().strip().split('\\n')\n",
    "    lines_neg1 = open('/home/sysu502/Public/duxin/weakly/Large_Scale_Sentiment_Classification_Data_v0.2/Weakly_labeled_data_1.1M/camera_negative.csv'\n",
    "                     , encoding='utf-8').read().strip().split('\\n')\n",
    "    lines_pos2 = open('/home/sysu502/Public/duxin/weakly/Large_Scale_Sentiment_Classification_Data_v0.2/Weakly_labeled_data_1.1M/cellphone_positive.csv'\n",
    "                     , encoding='utf-8').read().strip().split('\\n')\n",
    "    lines_neg2 = open('/home/sysu502/Public/duxin/weakly/Large_Scale_Sentiment_Classification_Data_v0.2/Weakly_labeled_data_1.1M/cellphone_negative.csv'\n",
    "                     , encoding='utf-8').read().strip().split('\\n')\n",
    "    lines_pos3 = open('/home/sysu502/Public/duxin/weakly/Large_Scale_Sentiment_Classification_Data_v0.2/Weakly_labeled_data_1.1M/laptop_positive.csv'\n",
    "                     , encoding='utf-8').read().strip().split('\\n')\n",
    "    lines_neg3 = open('/home/sysu502/Public/duxin/weakly/Large_Scale_Sentiment_Classification_Data_v0.2/Weakly_labeled_data_1.1M/laptop_negative.csv'\n",
    "                     , encoding='utf-8').read().strip().split('\\n')\n",
    "    \n",
    "    lines_pos = lines_pos1 + lines_pos2 + lines_pos3\n",
    "    lines_neg = lines_neg1 + lines_neg2 + lines_neg3\n",
    "    \n",
    "    lines = open('/home/sysu502/Public/duxin/weakly/Large_Scale_Sentiment_Classification_Data_v0.2/Labeled_data_11754/new_11754.csv'\n",
    "                     , encoding='gbk').read().strip().split('\\n')\n",
    "    \n",
    "    pairs_classify = [normalizeString(s) for s in lines]\n",
    "    \n",
    "    pairs_pos = [normalizeString(s) for s in lines_pos]\n",
    "    pairs_neg = [normalizeString(s) for s in lines_neg]\n",
    "\n",
    "    vocab = {}\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Take Word To Vec\")\n",
    "\n",
    "    final_embedding = np.array(np.load(\"/home/sysu502/Public/duxin/weakly/embedding/Vector_word_embedding_all.npy\"))\n",
    "    # final_embedding = np.delete(final_embedding, 60905, 0)\n",
    "    # print(final_embedding[60905])\n",
    "\n",
    "    maxlen = 0\n",
    "    bb = []\n",
    "\n",
    "    def word2idx(sentence, vocab, maxlen, bb):\n",
    "        items = sentence.strip().split()\n",
    "        if len(items) > maxlen:\n",
    "            maxlen = len(items)\n",
    "            bb = items\n",
    "        for word in items:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        return maxlen, bb\n",
    "\n",
    "    for line in pairs_classify:\n",
    "        maxlen, bb = word2idx(line, vocab, maxlen, bb)\n",
    "    \n",
    "    for line in pairs_pos:\n",
    "        maxlen, bb = word2idx(line, vocab, maxlen, bb)\n",
    "\n",
    "    for line in pairs_neg:\n",
    "        maxlen, bb = word2idx(line, vocab, maxlen, bb)\n",
    "\n",
    "    input_sentence_1 = 108947 + np.zeros((len(pairs_pos), 302))\n",
    "    input_sentence_1 = input_sentence_1.astype(np.int)\n",
    "    input_sentence_2 = 108947 + np.zeros((len(pairs_neg), 302))\n",
    "    input_sentence_2 = input_sentence_2.astype(np.int)\n",
    "\n",
    "    def sentence2vec(sentence, vocab, wordindex):\n",
    "        items = sentence.strip().split()\n",
    "        length = len(items)\n",
    "        for word in items:\n",
    "            wordindex.append(vocab[word])\n",
    "        return length, wordindex\n",
    "\n",
    "    def cal_sentence_index():\n",
    "        for line in range(len(pairs_pos)):\n",
    "            wordindex = []\n",
    "            length, wordindex = sentence2vec(pairs_pos[line], vocab, wordindex)\n",
    "            input_sentence_1[line][0] = length\n",
    "            input_sentence_1[line][1] = 10\n",
    "            input_sentence_1[line][2:length + 2] = np.array(wordindex)\n",
    "\n",
    "        for line in range(len(pairs_neg)):\n",
    "            wordindex = []\n",
    "            length, wordindex = sentence2vec(pairs_neg[line], vocab, wordindex)\n",
    "            input_sentence_2[line][0] = length\n",
    "            input_sentence_2[line][1] = 10\n",
    "            input_sentence_2[line][2:length + 2] = np.array(wordindex)\n",
    "        return input_sentence_1, input_sentence_2\n",
    "\n",
    "    cal_sentence_index()\n",
    "    \n",
    "#     add = -1 + 2*np.random.random(300)\n",
    "    add = np.zeros(300)\n",
    "    final_embedding = np.row_stack((final_embedding, add))\n",
    "    \n",
    "    np.random.shuffle(input_sentence_1)\n",
    "    np.random.shuffle(input_sentence_2)\n",
    "\n",
    "    input_pos_train = input_sentence_1[:int(len(input_sentence_1) * 0.7), :]\n",
    "    input_neg_train = input_sentence_2[:int(len(input_sentence_2) * 0.7), :]\n",
    "\n",
    "    input_pos_test = input_sentence_1[int(len(input_sentence_1) * 0.7):, :]\n",
    "    input_neg_test = input_sentence_2[int(len(input_sentence_2) * 0.7):, :]\n",
    "\n",
    "    def random_sample(matrix, sample_size):\n",
    "        matrix_after = []\n",
    "        sample_index = np.random.randint(0, len(matrix), sample_size)\n",
    "        for i in sample_index:\n",
    "            # np.row_stack((matrix_after, matrix[i]))\n",
    "            matrix_after.append(matrix[i])\n",
    "        return np.array(matrix_after)\n",
    "\n",
    "    train_pos_1 = random_sample(input_pos_train, sample_size)\n",
    "    train_pos_2 = random_sample(input_pos_train, sample_size)\n",
    "    train_pos_neg = random_sample(input_neg_train, sample_size)\n",
    "    train_neg_1 = random_sample(input_neg_train, sample_size)\n",
    "    train_neg_2 = random_sample(input_neg_train, sample_size)\n",
    "    train_neg_pos = random_sample(input_pos_train, sample_size)\n",
    "\n",
    "    train_dim1 = np.vstack((train_pos_1, train_neg_1))\n",
    "    train_dim2 = np.vstack((train_pos_2, train_neg_2))\n",
    "    train_dim3 = np.vstack((train_pos_neg, train_neg_pos))\n",
    "\n",
    "    def read_data(dim_1, dim_2, dim_3):\n",
    "        all_data = []\n",
    "        for idx in range(len(dim_1)):\n",
    "            items = torch.from_numpy(dim_1[idx])\n",
    "            items1 = torch.from_numpy(dim_2[idx])\n",
    "            items2 = torch.from_numpy(dim_3[idx])\n",
    "            data = {\n",
    "                'input1': items,\n",
    "                'input2': items1,\n",
    "                'input3': items2\n",
    "            }\n",
    "            all_data.append(data)\n",
    "        return all_data\n",
    "\n",
    "    all_data = DataProcess(read_data(train_dim1, train_dim2, train_dim3))\n",
    "    return all_data, final_embedding, np.array(input_pos_test[0:8000, :]), np.array(input_neg_test[0:8000, :])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    instructor = Instructor()\n",
    "    # instructor.beginTrain()\n",
    "    instructor.beginTrain_lstm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
